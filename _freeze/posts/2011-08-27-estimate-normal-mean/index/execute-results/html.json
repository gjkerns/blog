{
  "hash": "abcfdd00f89ad84bcfe3342446a5975c",
  "result": {
    "markdown": "---\ntitle: \"Estimating a normal mean with a cauchy prior\"\ndescription: \"This is an example (with a Bayesian spin) illustrating one way to compute a complicated expectation (that is, an integral) by the Monte Carlo method.\"\nauthor: \"G. Jay Kerns\"\ndate: \"2011-08-27\"\ncategories: [R, code, Bayesian]\nimage: \"fig-yplot-1.png\"\n---\n\n\n## The setup \n\nWhen doing statistics the Bayesian way, we are sometimes bombarded with complicated integrals that do not lend themselves to closed-form solutions. This used to be a problem. Nowadays, not so much. This post illustrates how a person can use the Monte Carlo method (and R) to get a good estimate for an integral that might otherwise look unwieldy at first glance. Of course, in this example, the integral isn't very complicated. But the /method/ works the same, regardless of the mess in which we find ourselves. The current example is derived from one in [Monte Carlo Statistical Methods](http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-21239-5) by Robert/Casella (in Chapter 4). For that matter, check out their [Introducing Monte Carlo Methods with R](http://www.springer.com/statistics/computanional+statistics/book/978-1-4419-1575-7).\n\nSuppose we have one observation $X \\sim N(\\theta,1)$ but we have a (robust) prior distribution on $\\theta$, namely, $\\theta \\sim \\mathrm{Cauchy}(0,1)$. We would like to update our beliefs about $\\theta$ based on the information provided by $x$. So our likelihood is \n$$\nf(x\\vert\\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n$$\nand our prior is \n$$\ng(\\theta) = \\frac{1}{\\pi}\\frac{1}{(1 + \\theta^{2})}.\n$$\nThe posterior distribution is proportional to the likelihood times prior, that is, \n$$\ng(\\theta\\vert x) \\propto \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})},\n$$\nwith the proportionality constant being the reciprocal of \n$$\nC = \\int \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta.\n$$\nOur point estimate (or best guess) for $\\theta$ will be just the *posterior mean*, given by \n$$\n\\mathbb{E} (\\theta \\vert \\mbox{data}) = \\frac{ \\int \\theta \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta }{C}.\n$$\n\n\nWe notice that the integrand for $C$ looks like *something* times a Cauchy PDF, where the *something* (let's call it $h$) is \n$$\nh(\\theta) = \\pi \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n$$\nso one way to use the Monte Carlo method follows. \n\n### Procedure: \n\nGiven observed data $X=x$, \n\n   1. Simulate a bunch of Cauchys, $\\theta_{1},\\theta_{2},\\ldots,\\theta_{m}$, say. \n   2. Estimate the integral in the denominator with \n      $$\\frac{1}{m}\\sum_{i=1}^{m} \\pi\\exp\\left[-\\frac{1}{2}(x - \\theta_{i})^2 \\right].$$ \n   3. Estimate the integral in the numerator with \n      $$\\frac{1}{m}\\sum_{i=1}^{m}* \\pi\\theta{i} \\exp \\left[-\\frac{1}{2}(x - \\theta_{i})^2  \\right].$$ \n   4. Take the ratio, and we're done.\n\nThe [Strong Law of Large Numbers](http://en.wikipedia.org/wiki/Strong_Law_of_Small_Numbers) says that the averages in 2 and 3 both converge to where they should, so the ratio should converge to the right place as well.\n\n## How to do it with R \n\nThe following is an R script which does the above. For laughs, let's suppose that we observed $X=3$.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)       # makes the experiment reproducible\nm <- 2000         # number of simulated values\nx <- 3            # observed data\n\n# Now simulate some random variables\n\ntheta <- rcauchy(m)                   # simulate m standard Cauchys \nh <- pi * exp(-0.5*(x - theta)^2)     # who wants to write this over and over\n\nConstant <- mean(h)                   # estimate normalizing constant \npost.mean <- mean(theta * h)/mean(h)  # estimate posterior mean #+end_src\n```\n:::\n\n\n\n## At the command prompt\n\nAfter copy-pasting the above into an R session we can see what the results were with an additional \n\n\n::: {.cell}\n\n```{.r .cell-code}\nConstant\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 0.3724711\n```\n:::\n\n```{.r .cell-code}\npost.mean\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.334232\n```\n:::\n:::\n\n\nFor this simple example we can actually calculate what the true values are (to machine precision) with the following: for the constant $C$ we get \n\n\n::: {.cell}\n\n```{.r .cell-code}\nf <- function(x) exp(-0.5*(x - 3)^2)/(1 + x^2)\nintegrate(f, -Inf, Inf)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n0.3416549 with absolute error < 1.3e-07\n```\n:::\n:::\n\n\nso our estimate of $C$ overshot the mark by about 0.03, and in the posterior mean case we get \n\n\n::: {.cell}\n\n```{.r .cell-code}\ng <- function(x) x * f(x)\nintegrate(g, -Inf, Inf)$value / integrate(f, -Inf, Inf)$value\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 2.285139\n```\n:::\n:::\n\n\nso our estimate of the posterior mean was around 0.05 too high. If we would like to get better estimates, we could increase the value of `m = 2000` to something higher (assuming these things are actually converging someplace).\n\n## Are we waiting long enough? \n\nOur estimates were a little bit off; we might like to take a look at a plot to see how we're doing -- is this thing really converging like we'd expect? We can look at a running average plot to assess convergence. If the plot bounces around indeterminably, that's bad, but if it settles down to a finite constant, that's better. Here's a quick way to check this out with `ggplot` graphics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrc <- cumsum(h)/seq_along(h)          # running mean of C \nrpm <- cumsum(h * theta)/cumsum(h)    # running posterior mean\n```\n:::\n\n\nNow we plot the results.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\nlibrary(grid)\nA <- data.frame(iter = 1:m, rc = rc, rpm=rpm)\nlibrary(reshape)\nA.short <- melt(A[3:200, ], id=\"iter\")\na <- ggplot(A.short, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"First 200\")\nA.long <- melt(A, id=\"iter\")\nb <- ggplot(A.long, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"All 2000 iterations\")\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(1, 2, widths = unit(c(3,5),\"null\"))))\nvplayout <- function(x, y)\nviewport(layout.pos.row = x, layout.pos.col = y)\nprint(a, vp = vplayout(1, 1))\nprint(b, vp = vplayout(1, 2))\n```\n\n::: {.cell-output-display}\n![Running averages for assessing convergence of the estimators.](index_files/figure-html/fig-yplot-1.png){#fig-yplot width=672}\n:::\n:::\n\n\n\nIn this example, the estimates look to be still unstable at around `m = 200`, but by the time we reach `m = 2000` they look to have pretty much settled down. Here we knew what the true values were, so we could tell immediately how well we were doing. On the battlefield we will not be so lucky. In general, with Monte Carlo estimates like these it is wise to take a look at some plots to judge the behavior of our estimators. If our plot looks more like the one on the left, then we should consider increasing the sample size. If our plot looks more like the one on the right, then maybe we would be satisfied with \"close enough\". (We can always wait longer, tight purse-strings notwithstanding.)\n\n## Other approaches \n\nWhen we were looking to estimate $C$ we noticed that the integrand was *something* times a Cauchy distribution. If we look again, we can see that the same integrand also looks like a *normal* distribution times *something*. So, another approach would be to simulate a bunch of normals and average the new *somethings*. Do we get the same answer (in the limit)?\n\nYes, of course. It turns out, the approach simulating normals does a little bit better than the one simulating Cauchys, but they're really pretty close. Check out chapter 4 of [Monte Carlo Statistical Methods](http://www.springer.com/statistics/statistical+theory+and+methods/book/978-0-387-21239-5) for discussion on this.\n\n## Where to find more...\n\nThe above is a variant of an example we did in [STAT 5840, Statistical Computing](https://github.com/gjkerns/STAT5840). The entire course is available online at [GitHub](https://github.com/gjkerns/STAT5840). Go to the Downloads for a `.zip` file or `.tar.gz`. Or, if you have [git](http://git-scm.com/) installed, you can get (git? har har) it all with\n\n```\ngit clone git://github.com/gjkerns/STAT5840.git\n```\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}