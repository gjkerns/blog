[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics, plain and sample",
    "section": "",
    "text": "R\n\n\ncode\n\n\nClassical\n\n\n\n\nA colleague is working on a paper and we recently got together to work through some of the statistical issues.\n\n\n\n\n\n\nJan 20, 2012\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nR\n\n\ncode\n\n\nBayesian\n\n\n\n\nThis is an example (with a Bayesian spin) illustrating one way to compute a complicated expectation (that is, an integral) by the Monte Carlo method.\n\n\n\n\n\n\nAug 27, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nThis is the first post. A mission statement, of sorts.\n\n\n\n\n\n\nAug 23, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a blog written by me. It isn’t meant as a political platform, personal diary, or much of anything more than my prattling on about topics that interest me in statistics, data science, and data analytics."
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html",
    "href": "posts/2011-08-23-maiden/index.html",
    "title": "Maiden Voyage",
    "section": "",
    "text": "The bottom line: with this setup I can effortlessly do R code like this:\n\nrnorm(10)\n\n [1] -0.8417498  1.5661733  1.4517939  0.8075502 -1.3006587  1.4853693\n [7]  0.0157239  1.1162398  1.4359540 -1.3781027\n\n\nAnd can include plots like this:\n\nx = rnorm(100) \ny = rnorm(100)\nplot(y ~ x)\n\n\n\n\nFigure 1: A plot to get things started.\n\n\n\n\nall housed inside a simple, dynamic text file that I can edit with Emacs and can version-control with git. On top of all this, I get LaTeX formatting in HTML via MathJax. Life is good."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "",
    "text": "When doing statistics the Bayesian way, we are sometimes bombarded with complicated integrals that do not lend themselves to closed-form solutions. This used to be a problem. Nowadays, not so much. This post illustrates how a person can use the Monte Carlo method (and R) to get a good estimate for an integral that might otherwise look unwieldy at first glance. Of course, in this example, the integral isn’t very complicated. But the /method/ works the same, regardless of the mess in which we find ourselves. The current example is derived from one in Monte Carlo Statistical Methods by Robert/Casella (in Chapter 4). For that matter, check out their Introducing Monte Carlo Methods with R.\nSuppose we have one observation \\(X \\sim N(\\theta,1)\\) but we have a (robust) prior distribution on \\(\\theta\\), namely, \\(\\theta \\sim \\mathrm{Cauchy}(0,1)\\). We would like to update our beliefs about \\(\\theta\\) based on the information provided by \\(x\\). So our likelihood is \\[\nf(x\\vert\\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] and our prior is \\[\ng(\\theta) = \\frac{1}{\\pi}\\frac{1}{(1 + \\theta^{2})}.\n\\] The posterior distribution is proportional to the likelihood times prior, that is, \\[\ng(\\theta\\vert x) \\propto \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})},\n\\] with the proportionality constant being the reciprocal of \\[\nC = \\int \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta.\n\\] Our point estimate (or best guess) for \\(\\theta\\) will be just the posterior mean, given by \\[\n\\mathbb{E} (\\theta \\vert \\mbox{data}) = \\frac{ \\int \\theta \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta }{C}.\n\\]\nWe notice that the integrand for \\(C\\) looks like something times a Cauchy PDF, where the something (let’s call it \\(h\\)) is \\[\nh(\\theta) = \\pi \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] so one way to use the Monte Carlo method follows.\n\n\nGiven observed data \\(X=x\\),\n\nSimulate a bunch of Cauchys, \\(\\theta_{1},\\theta_{2},\\ldots,\\theta_{m}\\), say.\nEstimate the integral in the denominator with \\[\\frac{1}{m}\\sum_{i=1}^{m} \\pi\\exp\\left[-\\frac{1}{2}(x - \\theta_{i})^2 \\right].\\]\nEstimate the integral in the numerator with \\[\\frac{1}{m}\\sum_{i=1}^{m}* \\pi\\theta{i} \\exp \\left[-\\frac{1}{2}(x - \\theta_{i})^2  \\right].\\]\nTake the ratio, and we’re done.\n\nThe Strong Law of Large Numbers says that the averages in 2 and 3 both converge to where they should, so the ratio should converge to the right place as well."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "How to do it with R",
    "text": "How to do it with R\nThe following is an R script which does the above. For laughs, let’s suppose that we observed \\(X=3\\).\n\nset.seed(1)       # makes the experiment reproducible\nm <- 2000         # number of simulated values\nx <- 3            # observed data\n\n# Now simulate some random variables\n\ntheta <- rcauchy(m)                   # simulate m standard Cauchys \nh <- pi * exp(-0.5*(x - theta)^2)     # who wants to write this over and over\n\nConstant <- mean(h)                   # estimate normalizing constant \npost.mean <- mean(theta * h)/mean(h)  # estimate posterior mean #+end_src"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "At the command prompt",
    "text": "At the command prompt\nAfter copy-pasting the above into an R session we can see what the results were with an additional\n\nConstant\n\n[1] 0.3724711\n\npost.mean\n\n[1] 2.334232\n\n\nFor this simple example we can actually calculate what the true values are (to machine precision) with the following: for the constant \\(C\\) we get\n\nf <- function(x) exp(-0.5*(x - 3)^2)/(1 + x^2)\nintegrate(f, -Inf, Inf)\n\n0.3416549 with absolute error < 1.3e-07\n\n\nso our estimate of \\(C\\) overshot the mark by about 0.03, and in the posterior mean case we get\n\ng <- function(x) x * f(x)\nintegrate(g, -Inf, Inf)$value / integrate(f, -Inf, Inf)$value\n\n[1] 2.285139\n\n\nso our estimate of the posterior mean was around 0.05 too high. If we would like to get better estimates, we could increase the value of m = 2000 to something higher (assuming these things are actually converging someplace)."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Are we waiting long enough?",
    "text": "Are we waiting long enough?\nOur estimates were a little bit off; we might like to take a look at a plot to see how we’re doing – is this thing really converging like we’d expect? We can look at a running average plot to assess convergence. If the plot bounces around indeterminably, that’s bad, but if it settles down to a finite constant, that’s better. Here’s a quick way to check this out with ggplot graphics.\n\nrc <- cumsum(h)/seq_along(h)          # running mean of C \nrpm <- cumsum(h * theta)/cumsum(h)    # running posterior mean\n\nNow we plot the results.\n\nlibrary(ggplot2)\nlibrary(grid)\nA <- data.frame(iter = 1:m, rc = rc, rpm=rpm)\nlibrary(reshape)\nA.short <- melt(A[3:200, ], id=\"iter\")\na <- ggplot(A.short, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"First 200\")\nA.long <- melt(A, id=\"iter\")\nb <- ggplot(A.long, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"All 2000 iterations\")\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(1, 2, widths = unit(c(3,5),\"null\"))))\nvplayout <- function(x, y)\nviewport(layout.pos.row = x, layout.pos.col = y)\nprint(a, vp = vplayout(1, 1))\nprint(b, vp = vplayout(1, 2))\n\n\n\n\nFigure 1: Running averages for assessing convergence of the estimators.\n\n\n\n\nIn this example, the estimates look to be still unstable at around m = 200, but by the time we reach m = 2000 they look to have pretty much settled down. Here we knew what the true values were, so we could tell immediately how well we were doing. On the battlefield we will not be so lucky. In general, with Monte Carlo estimates like these it is wise to take a look at some plots to judge the behavior of our estimators. If our plot looks more like the one on the left, then we should consider increasing the sample size. If our plot looks more like the one on the right, then maybe we would be satisfied with “close enough”. (We can always wait longer, tight purse-strings notwithstanding.)"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Other approaches",
    "text": "Other approaches\nWhen we were looking to estimate \\(C\\) we noticed that the integrand was something times a Cauchy distribution. If we look again, we can see that the same integrand also looks like a normal distribution times something. So, another approach would be to simulate a bunch of normals and average the new somethings. Do we get the same answer (in the limit)?\nYes, of course. It turns out, the approach simulating normals does a little bit better than the one simulating Cauchys, but they’re really pretty close. Check out chapter 4 of Monte Carlo Statistical Methods for discussion on this."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Where to find more…",
    "text": "Where to find more…\nThe above is a variant of an example we did in STAT 5840, Statistical Computing. The entire course is available online at GitHub. Go to the Downloads for a .zip file or .tar.gz. Or, if you have git installed, you can get (git? har har) it all with\ngit clone git://github.com/gjkerns/STAT5840.git"
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html",
    "href": "posts/2012-01-20-power-sample-size/index.html",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "",
    "text": "One of my colleagues is an academic physical therapist (PT), and he’s working on a paper to his colleagues related to power, sample size, and navigating the thicket of trouble that surrounds those two things. We recently got together to walk through some of the issues, and I thought I would share some of the wildlife we observed along the way. If you just want the code and don’t care about the harangue, see this gist on GitHub."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#the-problem",
    "href": "posts/2012-01-20-power-sample-size/index.html#the-problem",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "The problem",
    "text": "The problem\nSuppose you are a PT, and you’ve come up with a brand new exercise method that you think will decrease neck pain, say. How can you demonstrate that your method is effective? Of course, you collect data and show that people using your method have significantly lower neck pain than those from a control group.\nThe standard approach in the PT literature to analyze said data is repeated measures ANOVA. (Yes, those guys should really be using mixed-effects models, but those haven’t quite taken off yet.) There are two groups: the “Treatment” group does your new exercise method, and a “Sham” group does nothing (or just the placebo exercise method). For each Subject, you measure their pain at time 0, 15 minutes, 48 hours, and 96 hours. Pain is measured by an index (there are several); the one we’re using is something called NDI, which stands for “Neck Disability Index”. The index ranges from 0 to 100 (more on this later). There is some brief information about the index here.\nNow comes the question: how many people should you recruit for your study? The answer is: it depends. “On what?” Well, it depends on how good the statistical test is, and how good your method is, but more to the point, it depends on the effect size, that is, how far apart the two groups are, given that the method actually works.\nI encounter some variant of this question a lot. I used to go look for research papers where somebody’s worked out the F-test and sample sizes required, and pore over tables and tables. Then I resorted to online calculators (the proprietary versions were too expensive for my department!), which are fine, but they all use different notation and it takes a lot of time poring through documentation (which is often poor, pardon the pun) to recall how it works. And I was never really sure whether I’d got it right, or if I had screwed up with a parameter somewhere.\nSome of the calculators advertise Cohen’s effect sizes, which are usually stated something like “small”, “medium”, and “large”, with accompanying numerical values. Russell Lenth calls these “T-shirt effect sizes”. I agree with him.\nNowadays the fashionable people say, “Just run a simulation and estimate the power,” but the available materials online are scantly detailed. So my buddy and I worked it all out from start to finish for this simple example, in the hopes that by sharing this information people can get a better idea of how to do it the right way, the first time."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#how-to-attack-it",
    "href": "posts/2012-01-20-power-sample-size/index.html#how-to-attack-it",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "How to attack it",
    "text": "How to attack it\nThe avenue of attack is simple: for a given sample size,\n\nuse prior research and practitioner experience to decide what difference would be “meaningful” to detect,\nsimulate data consistent with the above difference and run the desired statistical test to see whether or not it rejected, and\nrepeat step 2 hundreds of times. An estimate of the power (for that sample size) is the proportion of times that the test rejected.\n\nIf the power isn’t high enough, then increase the given sample size and start over. The value we get is just an estimate of the power, but we can increase the precision of our estimate by increasing the number of repetitions in step 3.\nWhat you find when you start down this path is that there is a lot of information required to be able to answer the question. Of course, this information had been hiding behind the scenes all along, even with those old research papers and online calculators, but the other methods make it easy to gloss over the details, or they’re so complicated that researchers will give up and fall back to something like Cohen’s T-shirt effect sizes."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#now-for-the-legwork",
    "href": "posts/2012-01-20-power-sample-size/index.html#now-for-the-legwork",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Now for the legwork",
    "text": "Now for the legwork\nThe details we need include: A) prior knowledge of how average pain decreases for people in the Sham group, B) some idea about the variability of scores, C) how scores would be correlated with one another over time, and D) how much better the Treat group would need to be in order for the new procedure to be considered clinically meaningful.\nAs a first step, the PT sat down and filled in the following table.\n\n\n\n\n0 hrs\n15 min\n48 hrs\n96 hrs\n\n\n\n\nTreat\n37\n32\n20\n15\n\n\nSham\n37\n32\n25\n22\n\n\n\nAll of the entries in the above table represent population mean NDI scores for people in the respective groups at the respective measurement times, and were filled in based on prior research and educated guesses by the PT. It was known from other studies that NDI scores have a standard deviation of around 12, and those have been observed to decrease over time.\nNote: we could have assumed a simpler model for the means. For example, we could have assumed that mean NDI was linear, with possibly different slopes/intercepts for the Treat/Sham groups. Prior info available to the PT said that such an assumption wasn’t reasonable for this example.\nRepeated measures designs assume sphericity for the exact F tests to hold, so we need to specify a variance for the differences, \\(\\mathrm{Var}(X_{i} - X_{j})\\), and sphericity says this variance should be the same for all time points \\(i\\) and \\(j\\). As it turns out, this last choice implicitly determines all of the remaining covariance structure. We set this standard deviation to \\(9\\)."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#finally-we-do-some-coding",
    "href": "posts/2012-01-20-power-sample-size/index.html#finally-we-do-some-coding",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Finally we do some coding",
    "text": "Finally we do some coding\nWe are now ready to turn on the computer. We first intialize the parameters we’ll need, next we set up the independent variable data, then we do the simulation, and finally we rinse-and-repeat. Let’s go.\n\nset.seed(1)\nnPerGroup <- 10\nnTime     <- 4\nmuTreat   <- c(37, 32, 20, 15)\nmuSham    <- c(37, 32, 25, 22)\nstdevs    <- c(12, 10, 8, 6)\nstdiff    <- 9\nnSim      <- 500\n\nAll of the above should be self-explanatory. Next comes setting up the data - creatively named theData - for the independent variables. Just for the sake of argument I used code to generate the data frame, but we wouldn’t have had to. We could have imported an external text file had we wished.\n\nSubject <- factor(1:(nPerGroup*2))\nTime <- factor(1:nTime, labels = c(\"0min\", \"15min\", \"48hrs\", \"96hrs\"))\n\ntheData <- expand.grid(Time, Subject)\nnames(theData) <- c(\"Time\", \"Subject\")\n\ntmp <- rep(c(\"Treat\", \"Sham\"), each = nPerGroup * nTime)\ntheData$Method <- factor(tmp)\n\nAgain, the above should be self-explanatory for the most part. The data are in “long” form, where each subject appears over multiple rows. In fact, let’s take a look at the data frame to make sure it looks right.\n\nhead(theData)\n\n   Time Subject Method\n1  0min       1  Treat\n2 15min       1  Treat\n3 48hrs       1  Treat\n4 96hrs       1  Treat\n5  0min       2  Treat\n6 15min       2  Treat\n\n\nLookin’ good. Now for the fun part. We generate the single remaining column, the NDI scores. The repeated measures model is multivariate normal. The population covariance matrix is a little bit tricky, but it’s not too bad and to make things easy we’ll assume both groups have the same covariance. See the original paper by Huynh and Feldt for details.\n\n# to set up variance-covariance matrix\nones <- rep(1, nTime)\nA <- stdevs^2 %o% ones\nB <- (A + t(A) + (stdiff^2)*(diag(nTime) - ones %o% ones))/2\n\nWe simulate with the mvrnorm function from the MASS package.\n\nlibrary(MASS)\ntmp1 <- mvrnorm(nPerGroup, mu = muTreat, Sigma = B)\ntmp2 <- mvrnorm(nPerGroup, mu = muSham, Sigma = B)\ntheData$NDI <- c(as.vector(t(tmp1)), as.vector(t(tmp2)))\n\nNow that we have our data, we can run the test:\n\naovComp <- aov(NDI ~ Time*Method + Error(Subject/Time), theData)\nsummary(aovComp)\n\n\nError: Subject\n          Df Sum Sq Mean Sq F value Pr(>F)\nMethod     1  207.2   207.2   1.896  0.185\nResiduals 18 1967.5   109.3               \n\nError: Subject:Time\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nTime         3   4871  1623.5  42.709 2.79e-14 ***\nTime:Method  3    251    83.7   2.201   0.0985 .  \nResiduals   54   2053    38.0                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nTerrific! For these data, we observe a highly significant Time effect (this should be obvious given our table above), an insignificant Method fixed effect, and an insignificant Time:Method interaction. If we think about our model and what we’re interested in, it’s the interaction which we care about and that which we’d like to detect. If our significance level had been \\(\\alpha = 0.05\\), we would not have rejected this time, but who knows what would happen next time.\nNow it’s time to rinse-and-repeat, which we accomplish with the replicate function. Before we get there, though, let’s look at a plot. There are several relevant ones, but in the interest of brevity let’s satisfy ourselves with an interaction.plot:\n\nwith(theData, interaction.plot(Time, Method, NDI))\n\n\n\n\nEverything is going according to plan. There is definitely a Time effect (the lines both slope downward) but there isn’t any evidence of an interaction (the lines have similar slopes).\nOn to rinse-and-repeat, we first set up the function that runs the test once:\n\nrunTest <- function(){\n  tmp1 <- mvrnorm(nPerGroup, mu = muTreat, Sigma = B)\n  tmp2 <- mvrnorm(nPerGroup, mu = muSham, Sigma = B)\n  theData$NDI <- c(as.vector(t(tmp1)), as.vector(t(tmp2)))\n  aovComp <- aov(NDI ~ Time*Method + Error(Subject/Time), theData)  \n  b <- summary(aovComp)$'Error: Subject:Time'[[1]][2,5]\n  b < 0.05\n}\n\nand finally do the repeating:\n\nmean(replicate(nSim, runTest()))\n\n[1] 0.346\n\n\nWhoa! The power is 0.372? That’s pretty low. We recall that this is just an estimate of power - how precise is the estimate? The standard error of \\(\\hat{p}\\) is approximately \\(\\sqrt{\\hat{p}(1-\\hat{p})/n}\\), so in our case, our estimate’s standard error is approximately 0.022. That means we are approximately 95% confident that the true power at this particular alternative is covered by the interval \\([0.329,0.415]\\).\nStandard practice is to shoot for a power of around \\(\\beta = 0.80\\), so our power isn’t even close to what we’d need. We can increase power by increasing sample size (the parameter nPerGroup). A larger sample size means a longer time needed to run the simulation. Below are some results of running the above script at assorted sample sizes.\n\n\n\nnPerGroup\nPower (estimate)\nSE (approx)\n\n\n\n\n10\n0.372\n0.022\n\n\n20\n0.686\n0.021\n\n\n30\n0.886\n0.014\n\n\n\nNow we’re talking. It looks like somewhere between 20 and 30 subjects per group would be enough to detect the clinically meaningful difference proposed above with a power of 0.80.\nUnfortunately, the joke is on us. Because, as it happens, it’s no small order for a lone, practicing PT (around here) to snare 60 humans with neck pain for a research study. A person would need to be in (or travel to) a heavily populated area, and even then there would be dropout, people not showing up for subsequent appointments.\nSo what can we do?\n\nModify the research details. If we take a closer look at the table, there isn’t an expected difference in the means until 48 hours, so why not measure differently, say, at 0, 48, 96, and 144 hours? Is there something else about the measurement process we could change to decrease the variance?\nUse a different test. We are going with boilerplate repeated-measures ANOVA here. Is that really the best choice? What would happen if we tried the mixed-effects approach?\nTake a second look at the model. We should not only double-check our parameter choices, but rethink: is the repeated-measures model (multivariate normal) the most appropriate? Is it reasonable for the variance of differences at all time pairs to be identical? What about the covariance structure? There are others we could try, such as an autoregressive model (another arrow in the mixed-effects models’ quiver)."
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#other-things-to-keep-in-mind",
    "href": "posts/2012-01-20-power-sample-size/index.html#other-things-to-keep-in-mind",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "Other things to keep in mind",
    "text": "Other things to keep in mind\n\nThis example is simple enough to have done analytically; we didn’t have to simulate anything at all.\nEven if the example hadn’t been simple, we could still have searched for an approximate analytic solution which, if nothing else, might have given some insight into the power function’s behavior.\nWe could have adjusted all the means upward by 7 and nothing would have changed. We based our initial values on literature review and clinical expertise.\nWe didn’t bother with contrasts, functional means, or anything else. We just generated data consistent with our null and salient alternative and went on with our business.\nWe could have used whatever test we liked yet the method of attack would have been the same. Multiple comparisons, nested tests, nonparametric tests, whatever. As long as we include the full procedure in runTest, we will get valid estimates of power for that procedure at that alternative.\nWe need to be careful that the test we use (whatever it is) has its significance level controlled. This is easy to check in our example. We can set the means equal (muTreat = muSham) and run the simulation. We should get a power equal to 0.05 (within margin of error). Go ahead, check yourself. In fact, since we only care about the interaction, we could vertically offset the means by any fixed number, not necessarily zero.\nHad we not been careful with our stdevs, our simulated NDIs would have gone negative, particularly at the latter time points. That would not have been reasonable since NDI is nonnegative.\nSimulation is not a silver bullet.\nEffective simulation requires substantial investment of thought into both the probability model and the parameter settings.\nOur model had 13 parameters, and we had 4 more we didn’t even touch[fn:1]. A person could be forgiven for wondering how in the world all of those parameters can be expressively spun into a T-shirt effect size. (They can’t.)\nThe complexity can get out of control quickly. Simulation run times can take forever. The more complicated the model/test the worse it gets.\nInformative simulation demands literature review and content expertise as a prerequisite. Some researchers are unable (due to lack of existing/quality studies) or unwilling (for all sorts of reasons, not all of which are good) to help the statistician fill in the details. For the statistician, this is a problem. If you don’t know anything, then you can’t say anything.\nWe can address uncertainty in our parameter guesses with prior distributions on the parameters. This adds a layer of complexity to the simulation since we must first simulate the parameters before simulating the data. Sometimes there’s no other choice.\nTheory tells us that the standard research designs (including our current one) can usually be re-parameterized by a single non-centrality parameter which ultimately determines the power at any particular alternative. Following our nose, it suggests that our problem is simpler than we’re making it, that if we would just write down the non-centrality parameter (and the right numerator/denominator degrees of freedom), we’d be all set. Yep, we would. Good luck with all… that.\n\n[fn:1] John von Neumann once said, “With four parameters I can fit an elephant, and with five I can make him wiggle his trunk.”"
  },
  {
    "objectID": "posts/2012-01-20-power-sample-size/index.html#references",
    "href": "posts/2012-01-20-power-sample-size/index.html#references",
    "title": "Power and Sample Size for Repeated Measures ANOVA with R",
    "section": "References",
    "text": "References\n\nSee this question on CrossValidated which came up while I was working on this document (I might not have answered so quickly otherwise). Thanks to all who contributed to that discussion.\nConditions Under Which Mean Square Ratios in Repeated Measurements Designs Have Exact F-Distributions. Huynh Huynh and Leonard S. Feldt, Journal of the American Statistical Association, Vol. 65, No. 332 (Dec., 1970), pp. 1582-1589, stable link.\nI found this website while preparing for the initial meeting and got some inspiration from the discussion near the middle.\nThere are several papers on Russell Lenth’s webpage which are good reading.\nI also like this paper. Keselman, H. J., Algina, J. and Kowalchuk, R. K. (2001), The analysis of repeated measures designs: A review. British Journal of Mathematical and Statistical Psychology, 54: 1–20. doi: 10.1348/000711001159357\nMany of the concepts above are explained more formally in my Statistical Computing course which you can get on GitHub.\nTo learn more about Monte Carlo methods with R I recommend Introducing Monte Carlo Methods with R by Robert and Casella. I also like Statistical Computing with R by Rizzo which has a section about simulating power of statistical tests.\nFor the record, here is my sessionInfo.\n\n\nsessionInfo()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 19044)\n\nMatrix products: default\n\nlocale:\n[1] LC_COLLATE=English_United States.utf8 \n[2] LC_CTYPE=English_United States.utf8   \n[3] LC_MONETARY=English_United States.utf8\n[4] LC_NUMERIC=C                          \n[5] LC_TIME=English_United States.utf8    \n\nattached base packages:\n[1] stats     graphics  grDevices datasets  utils     methods   base     \n\nother attached packages:\n[1] MASS_7.3-58.1\n\nloaded via a namespace (and not attached):\n [1] digest_0.6.30   lifecycle_1.0.3 jsonlite_1.8.3  magrittr_2.0.3 \n [5] evaluate_0.18   rlang_1.0.6     stringi_1.7.8   cli_3.4.1      \n [9] renv_0.16.0     vctrs_0.5.1     rmarkdown_2.18  tools_4.2.2    \n[13] stringr_1.5.0   glue_1.6.2      xfun_0.35       yaml_2.3.6     \n[17] fastmap_1.1.0   compiler_4.2.2  htmltools_0.5.3 knitr_1.41"
  }
]