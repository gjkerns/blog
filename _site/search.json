[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics, plain and sample",
    "section": "",
    "text": "R\n\n\ncode\n\n\nBayesian\n\n\n\n\nLet’s do some Monte Carlo estimation and warm up the motor.\n\n\n\n\n\n\nAug 27, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\n\n\nA mission statement, of sorts.\n\n\n\n\n\n\nAug 23, 2011\n\n\nG. Jay Kerns\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "Since this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2011-08-23-maiden/index.html",
    "href": "posts/2011-08-23-maiden/index.html",
    "title": "Maiden Voyage",
    "section": "",
    "text": "The bottom line: with this setup I can effortlessly do R code like this:\n\nrnorm(10)\n\n [1]  1.62194330 -2.46046403  1.50199658  1.33395299  1.11540885  0.86499109\n [7] -0.66437719 -0.07203819  1.84956476  0.24651257\n\n\nAnd can include plots like this:\n\nx = rnorm(100) \ny = rnorm(100)\nplot(y ~ x)\n\n\n\n\nFigure 1: A plot to get things started.\n\n\n\n\nall housed inside a simple, dynamic text file that I can edit with Emacs and can version-control with git. On top of all this, I get LaTeX formatting in HTML via MathJax. Life is good."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "",
    "text": "When doing statistics the Bayesian way, we are sometimes bombarded with complicated integrals that do not lend themselves to closed-form solutions. This used to be a problem. Nowadays, not so much. This post illustrates how a person can use the Monte Carlo method (and R) to get a good estimate for an integral that might otherwise look unwieldy at first glance. Of course, in this example, the integral isn’t very complicated. But the /method/ works the same, regardless of the mess in which we find ourselves. The current example is derived from one in Monte Carlo Statistical Methods by Robert/Casella (in Chapter 4). For that matter, check out their Introducing Monte Carlo Methods with R.\nSuppose we have one observation \\(X \\sim N(\\theta,1)\\) but we have a (robust) prior distribution on \\(\\theta\\), namely, \\(\\theta \\sim \\mathrm{Cauchy}(0,1)\\). We would like to update our beliefs about \\(\\theta\\) based on the information provided by \\(x\\). So our likelihood is \\[\nf(x\\vert\\theta) = \\frac{1}{\\sqrt{2\\pi}}\\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] and our prior is \\[\ng(\\theta) = \\frac{1}{\\pi}\\frac{1}{(1 + \\theta^{2})}.\n\\] The posterior distribution is proportional to the likelihood times prior, that is, \\[\ng(\\theta\\vert x) \\propto \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})},\n\\] with the proportionality constant being the reciprocal of \\[\nC = \\int \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta.\n\\] Our point estimate (or best guess) for \\(\\theta\\) will be just the posterior mean, given by \\[\n\\mathbb{E} (\\theta \\vert \\mbox{data}) = \\frac{ \\int \\theta \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right] \\frac{1}{(1 + \\theta^{2})} \\mathrm{d} \\theta }{C}.\n\\]\nWe notice that the integrand for \\(C\\) looks like something times a Cauchy PDF, where the something (let’s call it \\(h\\)) is \\[\nh(\\theta) = \\pi \\exp \\left[-\\frac{1}{2}(x - \\theta)^2  \\right],\n\\] so one way to use the Monte Carlo method follows.\n\n\nGiven observed data \\(X=x\\),\n\nSimulate a bunch of Cauchys, \\(\\theta_{1},\\theta_{2},\\ldots,\\theta_{m}\\), say.\nEstimate the integral in the denominator with \\[\\frac{1}{m}\\sum_{i=1}^{m} \\pi\\exp\\left[-\\frac{1}{2}(x - \\theta_{i})^2 \\right].\\]\nEstimate the integral in the numerator with \\[\\frac{1}{m}\\sum_{i=1}^{m}* \\pi\\theta{i} \\exp \\left[-\\frac{1}{2}(x - \\theta_{i})^2  \\right].\\]\nTake the ratio, and we’re done.\n\nThe Strong Law of Large Numbers says that the averages in 2 and 3 both converge to where they should, so the ratio should converge to the right place as well."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#how-to-do-it-with-r",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "How to do it with R",
    "text": "How to do it with R\nThe following is an R script which does the above. For laughs, let’s suppose that we observed \\(X=3\\).\n\nset.seed(1)       # makes the experiment reproducible\nm <- 2000         # number of simulated values\nx <- 3            # observed data\n\n# Now simulate some random variables\n\ntheta <- rcauchy(m)                   # simulate m standard Cauchys \nh <- pi * exp(-0.5*(x - theta)^2)     # who wants to write this over and over\n\nConstant <- mean(h)                   # estimate normalizing constant \npost.mean <- mean(theta * h)/mean(h)  # estimate posterior mean #+end_src"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#at-the-command-prompt",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "At the command prompt",
    "text": "At the command prompt\nAfter copy-pasting the above into an R session we can see what the results were with an additional\n\nConstant\n\n[1] 0.3724711\n\npost.mean\n\n[1] 2.334232\n\n\nFor this simple example we can actually calculate what the true values are (to machine precision) with the following: for the constant \\(C\\) we get\n\nf <- function(x) exp(-0.5*(x - 3)^2)/(1 + x^2)\nintegrate(f, -Inf, Inf)\n\n0.3416549 with absolute error < 1.3e-07\n\n\nso our estimate of \\(C\\) overshot the mark by about 0.03, and in the posterior mean case we get\n\ng <- function(x) x * f(x)\nintegrate(g, -Inf, Inf)$value / integrate(f, -Inf, Inf)$value\n\n[1] 2.285139\n\n\nso our estimate of the posterior mean was around 0.05 too high. If we would like to get better estimates, we could increase the value of m = 2000 to something higher (assuming these things are actually converging someplace)."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#are-we-waiting-long-enough",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Are we waiting long enough?",
    "text": "Are we waiting long enough?\nOur estimates were a little bit off; we might like to take a look at a plot to see how we’re doing – is this thing really converging like we’d expect? We can look at a running average plot to assess convergence. If the plot bounces around indeterminably, that’s bad, but if it settles down to a finite constant, that’s better. Here’s a quick way to check this out with ggplot graphics.\n\nrc <- cumsum(h)/seq_along(h)          # running mean of C \nrpm <- cumsum(h * theta)/cumsum(h)    # running posterior mean\n\nNow we plot the results.\n\nlibrary(ggplot2)\nlibrary(grid)\nA <- data.frame(iter = 1:m, rc = rc, rpm=rpm)\nlibrary(reshape)\nA.short <- melt(A[3:200, ], id=\"iter\")\na <- ggplot(A.short, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"First 200\")\nA.long <- melt(A, id=\"iter\")\nb <- ggplot(A.long, aes(iter, value, colour=variable)) + geom_line() +\n      labs(title = \"All 2000 iterations\")\ngrid.newpage()\npushViewport(viewport(layout = grid.layout(1, 2, widths = unit(c(3,5),\"null\"))))\nvplayout <- function(x, y)\nviewport(layout.pos.row = x, layout.pos.col = y)\nprint(a, vp = vplayout(1, 1))\nprint(b, vp = vplayout(1, 2))\n\n\n\n\nFigure 1: Running averages for assessing convergence of the estimators.\n\n\n\n\nIn this example, the estimates look to be still unstable at around m = 200, but by the time we reach m = 2000 they look to have pretty much settled down. Here we knew what the true values were, so we could tell immediately how well we were doing. On the battlefield we will not be so lucky. In general, with Monte Carlo estimates like these it is wise to take a look at some plots to judge the behavior of our estimators. If our plot looks more like the one on the left, then we should consider increasing the sample size. If our plot looks more like the one on the right, then maybe we would be satisfied with “close enough”. (We can always wait longer, tight purse-strings notwithstanding.)"
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#other-approaches",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Other approaches",
    "text": "Other approaches\nWhen we were looking to estimate \\(C\\) we noticed that the integrand was something times a Cauchy distribution. If we look again, we can see that the same integrand also looks like a normal distribution times something. So, another approach would be to simulate a bunch of normals and average the new somethings. Do we get the same answer (in the limit)?\nYes, of course. It turns out, the approach simulating normals does a little bit better than the one simulating Cauchys, but they’re really pretty close. Check out chapter 4 of Monte Carlo Statistical Methods for discussion on this."
  },
  {
    "objectID": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "href": "posts/2011-08-27-estimate-normal-mean/index.html#where-to-find-more",
    "title": "Estimating a normal mean with a cauchy prior",
    "section": "Where to find more…",
    "text": "Where to find more…\nThe above is a variant of an example we did in STAT 5840, Statistical Computing. The entire course is available online at GitHub. Go to the Downloads for a .zip file or .tar.gz. Or, if you have git installed, you can get (git? har har) it all with\ngit clone git://github.com/gjkerns/STAT5840.git"
  }
]